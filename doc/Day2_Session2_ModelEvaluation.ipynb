{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "X = np.array([[0, 11], \n",
    "              [1, 22], \n",
    "              [2, 33], \n",
    "              [3, 44]])\n",
    "\n",
    "kf = KFold(n_splits=4, shuffle=True)\n",
    "\n",
    "for X_train, X_test in kf.split(X):\n",
    "    print(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stratified K-Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold \n",
    "\n",
    "X = np.array([[0, 11], \n",
    "              [1, 22], \n",
    "              [2, 33], \n",
    "              [3, 44]])\n",
    "y = np.array([0, 0, 1, 1])\n",
    "\n",
    "skf = StratifiedKFold(n_splits=2, shuffle=True)\n",
    "\n",
    "for X_train, X_test in skf.split(X,y):\n",
    "    print(X_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "X = np.array([[0, 11], \n",
    "              [1, 22], \n",
    "              [2, 33], \n",
    "              [3, 44]])\n",
    "y = np.array([0, 0, 1, 1])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.5, stratify=y)\n",
    "\n",
    "print(X_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "gs_data = iris.data\n",
    "gs_target = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "parameters = {'p':[1,2,3,4,5],\n",
    "              'n_neighbors':[3,5,7,11,13,15,17]}\n",
    "knn = KNeighborsClassifier()\n",
    "gsc = GridSearchCV(knn, parameters)\n",
    "gsc.fit(gs_data, gs_target)\n",
    "gsc.cv_results_['mean_test_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsc.cv_results_['params']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = [0, 1, 0, 1]\n",
    "y_true = [0, 1, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "f1 = f1_score(y_true, y_pred,)\n",
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import matthews_corrcoef \n",
    "mcc = matthews_corrcoef(y_true, y_pred)\n",
    "mcc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROC & PRC Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading data from the iris dataset to generate ROC/PRC Curves\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "clf = OneVsRestClassifier(svm.SVC(kernel='linear', probability=True,\n",
    "                                 random_state=0))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.95,\n",
    "                                                    random_state=0)\n",
    "\n",
    "y_score = clf.fit(X_train, y_train).decision_function(X_test)\n",
    "\n",
    "#Converting multi-class labels to binary class labels\n",
    "y_true_svc = (y_test == 1).astype(int)\n",
    "\n",
    "#https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html?highlight=roc%20curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, precision_recall_curve, average_precision_score, auc\n",
    "def plotROC(yscore, true, title=None, outfile=None):\n",
    "    \"\"\"Generates a receiver operating characteristic\n",
    "        curve for the given prediction probabilities.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        yscore : list of lists\n",
    "        Probability scores.\n",
    "        \n",
    "        true : list of lists\n",
    "        True labels.\n",
    "        \n",
    "        title : str\n",
    "        The title of the confusion matrix.\n",
    "        \n",
    "        outfile : str\n",
    "        The destination of the .pdf file generated.\n",
    "        \"\"\"\n",
    "    fig = plt.figure()\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.0])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    \n",
    "    fpr, tpr, _ = roc_curve(true, yscore[:,1])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, label='(ROC AUC = %0.2f)' % (roc_auc),linewidth=2)\n",
    "\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    \n",
    "    if outfile:\n",
    "        pdfplot = PdfPages(outfile);\n",
    "        pdfplot.savefig(fig)\n",
    "        pdfplot.close()\n",
    "    \n",
    "\n",
    "def plotPRC(yscore, true, title=None, outfile=None):\n",
    "    \"\"\"Generates a precision recall curve for the\n",
    "        given prediction probabilities.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        yscore : list of lists\n",
    "        Probability scores.\n",
    "        \n",
    "        true : list of lists\n",
    "        True labels.\n",
    "        \n",
    "        title : str\n",
    "        The title of the confusion matrix.\n",
    "        \n",
    "        outfile : str\n",
    "        The destination of the .pdf file generated.\n",
    "        \"\"\"\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.0])\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    \n",
    "    precision, recall, _ =  precision_recall_curve(true, yscore[:,1])\n",
    "    prc_auc = average_precision_score(true, yscore[:,1])\n",
    "    plt.plot(recall, precision, label='(Avg. Precision = %0.2f)' % (prc_auc),linewidth=1)\n",
    "    \n",
    "    plt.legend(loc=\"lower right\")\n",
    "    if outfile:\n",
    "        pdfplot = PdfPages(outfile);\n",
    "        pdfplot.savefig(fig)\n",
    "        pdfplot.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotROC(y_score, y_true_svc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotPRC(y_score, y_true_svc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
